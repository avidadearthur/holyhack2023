{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\weeck\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I import and manipulate large datasets...</td>\n",
       "      <td>Pandas Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the best way to perform data preproces...</td>\n",
       "      <td>scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I package my application and its depen...</td>\n",
       "      <td>Docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a tool that can help me create intera...</td>\n",
       "      <td>Tableau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can I use Python to create visualizations and ...</td>\n",
       "      <td>Pandas Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>How can I handle imbalanced classes in my data...</td>\n",
       "      <td>scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>What is the most reliable way to deploy my app...</td>\n",
       "      <td>Docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>How can I perform statistical analysis and hyp...</td>\n",
       "      <td>Pandas Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>What is the best way to handle text data and p...</td>\n",
       "      <td>scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>How can I easily share my visualizations and i...</td>\n",
       "      <td>Tableau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1705 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question           class\n",
       "0     How can I import and manipulate large datasets...  Pandas Library\n",
       "1     What is the best way to perform data preproces...    scikit-learn\n",
       "2     How can I package my application and its depen...          Docker\n",
       "3     Is there a tool that can help me create intera...         Tableau\n",
       "4     Can I use Python to create visualizations and ...  Pandas Library\n",
       "...                                                 ...             ...\n",
       "1700  How can I handle imbalanced classes in my data...    scikit-learn\n",
       "1701  What is the most reliable way to deploy my app...          Docker\n",
       "1702  How can I perform statistical analysis and hyp...  Pandas Library\n",
       "1703  What is the best way to handle text data and p...    scikit-learn\n",
       "1704  How can I easily share my visualizations and i...         Tableau\n",
       "\n",
       "[1705 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = []\n",
    "classes = []\n",
    "docs = []\n",
    "\n",
    "file = open('classClassification.txt', 'r')\n",
    "lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    elements = line.split(' (')\n",
    "    if elements[1][:-2] in ['scikit-learn', 'Pandas Library', 'Docker', 'Tableau']:\n",
    "        questions.append(elements[0])\n",
    "        classes.append(elements[1][:-2])\n",
    "        docs.append([elements[0], elements[1][:-2]])\n",
    "\n",
    "dict = {'question':questions, 'class':classes}\n",
    "df = pd.DataFrame(dict)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\tTrain\t\t0.958211\t0.958211\t0.958211\n",
      "\n",
      "Naive Bayes\tTrain\t\t0.947214\t0.947214\t0.947214\n",
      "\n",
      "Docker: 9.625%\tPandas Library: 14.208%\tTableau: 66.131%\tscikit-learn: 10.036%\t\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(text):\n",
    "    tokens = text.split(' ')\n",
    "    tokens = [t for t in tokens if not t in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Clans text\n",
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def print_frequency_dist(docs):\n",
    "    tokens = defaultdict(list)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_label = doc[1]\n",
    "        doc_text = clean_text(doc[0])\n",
    "        doc_tokens = get_tokens(doc_text)\n",
    "        tokens[doc_label].extend(doc_tokens)\n",
    "\n",
    "    for category_label, category_tokens in tokens.items():\n",
    "        print(category_label)\n",
    "        fd = FreqDist(category_tokens)\n",
    "        print(fd.most_common(20))\n",
    "\n",
    "def get_splits(docs):\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    pivot = int(.80 * len(docs))\n",
    "\n",
    "    for i in range(0, pivot):\n",
    "        X_train.append(docs[i][0])\n",
    "        Y_train.append(docs[i][1])\n",
    "    for i in range(pivot, len(docs)):\n",
    "        X_test.append(docs[i][0])\n",
    "        Y_test.append(docs[i][1])\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def evaluate_classifier(title, classifier, vectorizer, X_test, Y_test):\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    Y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "    precission = metrics.precision_score(Y_test, Y_pred, average='micro')\n",
    "    recall = metrics.recall_score(Y_test, Y_pred, average='micro')\n",
    "    f1 = metrics.f1_score(Y_test, Y_pred, average='micro')\n",
    "\n",
    "    print(\"%s\\t%f\\t%f\\t%f\\n\" % (title, precission, recall, f1))\n",
    "\n",
    "def train_classifier(docs):\n",
    "    X_train, X_test, Y_train, Y_test = get_splits(docs)\n",
    "\n",
    "    # Vectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english',\n",
    "                                 ngram_range=(1,3),\n",
    "                                 min_df=3,\n",
    "                                 analyzer='word')\n",
    "    dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Classifier\n",
    "    naive_bayes_classifier = MultinomialNB().fit(dtm, Y_train)\n",
    "\n",
    "    evaluate_classifier(\"Naive Bayes\\tTrain\\t\", naive_bayes_classifier, vectorizer, X_train, Y_train)\n",
    "    evaluate_classifier(\"Naive Bayes\\tTrain\\t\", naive_bayes_classifier, vectorizer, X_test, Y_test)\n",
    "\n",
    "    # Store classifier\n",
    "    clf_filename = 'classifier.pkl'\n",
    "    pickle.dump(naive_bayes_classifier, open(clf_filename, 'wb'))\n",
    "\n",
    "    # Store vectorizer\n",
    "    vec_filename = 'count_vectorizer.pkl'\n",
    "    pickle.dump(vectorizer, open(vec_filename, 'wb'))\n",
    "\n",
    "def print_prob(prediction, classes):\n",
    "    for i in range(len(classes)):\n",
    "        print(classes[i]+': ',end='')\n",
    "        print('{:.3%}'.format(prediction[i])+'\\t',end='')\n",
    "    print()\n",
    "\n",
    "def classify(text):\n",
    "    \n",
    "    clf_filename = 'classifier.pkl'\n",
    "    vec_filename = 'count_vectorizer.pkl'\n",
    "\n",
    "    nb_clf = pickle.load(open(clf_filename, 'rb'))\n",
    "    vectorizer = pickle.load(open(vec_filename, 'rb'))\n",
    "\n",
    "    pred = nb_clf.predict_proba(vectorizer.transform([text]))\n",
    "\n",
    "\n",
    "    print_prob(pred[0], nb_clf.classes_)\n",
    "\n",
    "\n",
    "#  print_frequency_dist(docs)\n",
    "train_classifier(docs)\n",
    "\n",
    "classify('I want to get insight on my chair data he feh')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
